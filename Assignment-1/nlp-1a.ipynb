import nltk
import spacy
import transformers

def read(file):
    with open(file, 'r') as f:
        text = f.read()
    return text
    
def tokenize(text):
    nltk_tokens = nltk_tokenize(text)
    print('\033[1m'+"\nnltk tokens:"+'\033[0m', nltk_tokens)
    spacy_tokens = spacy_tokenize(text)
    print('\033[1m'+"\nspacy tokens:"+'\033[0m', spacy_tokens)
    bert_tokens = bert_tokenize(text)
    print('\033[1m'+"\nbert tokens:"+'\033[0m', bert_tokens)
    
def nltk_tokenize(text):
    tokens = nltk.word_tokenize(text)
    return tokens

def spacy_tokenize(text):
    tz = spacy.load('en_core_web_sm') # loads English tokenizer, tagger, parser and NER
    doc = tz(text)
    tokens = [token.text for token in doc]
    return tokens

def bert_tokenize(text):
    tz = transformers.BertTokenizer.from_pretrained('bert-base-cased') # loads the BertTokenizer with the bert-base-cased model
    tokens = tz.tokenize(text)
    return tokens

def main():
    text = read('wsj_untokenized.txt')
    tokenize(text)
       
if __name__ == "__main__":
    main()
