{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nataliakoliou/NLP-Various-Implementations/blob/main/Assignment-2/Assignment-2c/nlp_2c.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP-Various Implementations | Text Classification with RNNs**\n",
        "\n",
        "**Overview:** In this part of the project, I trained several neural network models, including RNNs and LSTMs, with different architectures and hyperparameters to evaluate their performance on some simple classification tasks. For this purpose, I used the AG News Topic Classification and the IMDB movie review12 datasets. Overall, this work provided me with valuable hands-on experience in training neural networks and insight into the factors that affect their performance in text classification tasks. Through experimenting with different architectures and hyperparameters, I gained a deeper understanding of how these models operate and can be optimized to achieve high accuracy levels."
      ],
      "metadata": {
        "id": "c-6PFkBypCfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Import all the necessary modules**\n",
        "\n",
        "**Briefly:** ```time``` library provides functions for working with time-related tasks, ```torch``` library provides support for deep learning operations using tensors, ```random``` library provides tools for generating random numbers and ```pandas``` library provides data manipulation and analysis tools. Additionaly, ```nn``` module provides support for building neural networks, ```tqdm``` library provides a progress bar to track loops, ```defaultdict``` class provides a way to create a dictionary with default values for nonexistent keys, ```PrettyTable``` library provides a way to display data in a table format, ```functional``` module provides support for functional-style programming with neural networks, ```FiDataLoader``` class provides a way to load data in batches for training neural networks and ```get_tokenizer``` function provides a way to tokenize text. Finally, ```accuracy_score``` function provides a way to calculate the accuracy of a model and ```build_vocab_from_iterator``` function provides a way to build a vocabulary from an iterator of text."
      ],
      "metadata": {
        "id": "L-p_LzB2r7QV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from prettytable import PrettyTable\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import get_tokenizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ],
      "metadata": {
        "id": "_-67_ZIz2VLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Define and initialize the models' parameters**\n",
        "\n",
        "**Initialize the variables and hyperparameters of the classification models:** The set_device function checks if a CUDA-enabled GPU is available and sets the device accordingly. The tokenizer variable is set to tokenize the text data using the \"basic_english\" tokenizer. Both models and classes are lists that contain the different models and classes used for the classification process, whereas accuracies, parameters, and time_costs are empty lists that will be used to store evaluation metrics during the training process. Finally, the remaining variables are hyperparameters used for this training process.\n",
        "\n",
        "* The `models` listed in the models list are different types of neural network models that will be used for classification. Specifically, they are variations of recurrent neural networks (RNNs) and long short-term memory (LSTM) networks with different numbers of layers and types of connections between layers.\n",
        "\n",
        "* The `classes` list specifies the different categories or classes that the classification model will be trained to predict. In this case, the four classes are *World*, *Sports*, *Business*, and *Sci/Tech*, which suggest that the model is being trained to classify news articles or text documents into these four broad categories.\n",
        "\n",
        "* `MAX_WORDS = 25` sets a maximum limit for the number of words allowed in a text sample. This means that if a text sample contains more than 25 words, it will be truncated to 25 words before being fed into the classification model."
      ],
      "metadata": {
        "id": "e_h3jKbStQvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_device(primary, secondary):\n",
        "    return torch.device(primary if torch.cuda.is_available() else secondary) # device used to perform the computations for the machine learning model\n",
        "\n",
        "device = set_device(\"cuda\",\"cpu\")\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "models = [\"1Uni-RNN\", \"1Bi-RNN\", \"2Bi-RNN\", \"1Uni-LSTM\", \"1Bi-LSTM\", \"2Bi-LSTM\"]; classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]; accuracies = []; parameters = []; time_costs = []\n",
        "MIN_FREQ = 10 ; MAX_WORDS = 25; EPOCHS = 15; LEARNING_RATE = 1e-3; BATCH_SIZE = 1024; EMBEDDING_DIM = 100; HIDDEN_DIM = 64; PADDED = \"<PAD>\"; UNKNOWN = \"<UNK>\""
      ],
      "metadata": {
        "id": "BkO4z42A2W-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Load and preprocess the training and testing datasets**\n",
        "\n",
        "**Create the training and testing datasets:** The load_dataset() function is used to load and preprocess a CSV file containing text data. It reads the CSV file using pandas, shuffles the rows (except the first one), and selects a subset of the data based on the given percent and mode arguments. It then combines the selected features into a single text column and returns a list of tuples, where each tuple contains the label and text data for each row of the dataset. The function is called twice to create train_dataset and test_dataset, which are used for training and testing a machine learning model.\n",
        "\n",
        "* `data.iloc[:1]` selects only the first row of the data, which typically contains column names that correspond to our models' classes. By selecting only the data rows for shuffling, we ensure that the column names remain in the first row (for later use) and are not affected by the shuffling process.\n",
        "\n",
        "* If `mode` is set to *start*, the first percent % of rows are selected, whereas if `mode` is set to *end*, the last percent % of rows are selected. The code calculates the starting and ending indexes based on the percent value and the total length of the dataset using integer division and multiplication."
      ],
      "metadata": {
        "id": "Z62FB_A1xjFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path, features, label, percent, mode):\n",
        "    data = pd.read_csv(path)\n",
        "    data = pd.concat([data.iloc[:1], data.iloc[1:].sample(frac=1)], ignore_index=True)  # shuffle all rows except the first one\n",
        "    if mode == 'start':\n",
        "        end_index = int(len(data) * (percent / 100))\n",
        "        data = data.iloc[:end_index]\n",
        "    elif mode == 'end':\n",
        "        start_index = int(len(data) * ((100 - percent) / 100))\n",
        "        data = pd.concat([data.iloc[0:0], data.iloc[start_index:]], ignore_index=True)\n",
        "    text = data[features].astype(str).agg(' '.join, axis=1)\n",
        "    return [(data[label][i], text[i]) for i in range(len(data))]\n",
        "\n",
        "train_dataset, test_dataset = load_dataset(\"train.csv\", [\"Title\",\"Description\"], \"Class Index\", 100, \"start\"), load_dataset(\"test.csv\", [\"Title\",\"Description\"], \"Class Index\", 100, \"start\")"
      ],
      "metadata": {
        "id": "sEE-_zcH27Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Build PyTorch DataLoaders for efficient model training and testing**\n",
        "\n",
        "**Generate PyTorch DataLoaders for the classification process:** The generate_loader() function takes in a dataset, a maximum number of words, a batch size, and a shuffle flag, and returns a PyTorch DataLoader object with the specified parameters. The collate_batch() function is used as a custom collate function for the DataLoader, and it preprocesses the input data by tokenizing the text, padding the sequences with <PAD> tokens or truncating the sequences to a maximum length of max_words, and converting the data into PyTorch tensors. The DataLoader is then split into train_loader and test_loader, with train_loader being shuffled for better model training and test_loader being unshuffled for model evaluation."
      ],
      "metadata": {
        "id": "y8UGzQCv0sTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_loader(dataset, max_words, batch_size, shuffle):\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=lambda b: collate_batch(b, max_words))\n",
        "\n",
        "def collate_batch(batch, max_words):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1  # target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X] # type(X): list of lists\n",
        "    X = [tokens+([vocab['<PAD>']]* (max_words-len(tokens))) if len(tokens)<max_words else tokens[:max_words] for tokens in X]  # brings all samples to MAX_WORDS length - shorter texts are padded with <PAD> sequences, longer texts are truncated\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device)\n",
        "\n",
        "train_loader, test_loader = generate_loader(train_dataset, MAX_WORDS, BATCH_SIZE, True), generate_loader(test_dataset, MAX_WORDS, BATCH_SIZE, False)"
      ],
      "metadata": {
        "id": "lGyv6CVS3TLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(datasets, min_freq, padded, unknown):\n",
        "    vocab = build_vocab_from_iterator(tokenize(datasets), min_freq=min_freq, specials=[padded, unknown])\n",
        "    vocab.set_default_index(vocab[unknown])\n",
        "    return vocab\n",
        "\n",
        "def tokenize(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab([train_dataset, test_dataset], MIN_FREQ, PADDED, UNKNOWN)"
      ],
      "metadata": {
        "id": "5O_ZmNhb318J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_model(device, model, classes, vocab, embedding_dim, hidden_dim, num_layers, bidirectional, learning_rate, embeddings, freeze):\n",
        "    classifier = model(len(vocab), embedding_dim, hidden_dim, num_layers, bidirectional, len(classes), embeddings, freeze).to(device)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=learning_rate)\n",
        "    return classifier, loss_fn, optimizer\n",
        "  \n",
        "class RNN_model(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, bidirectional, output_dim, none, freeze):\n",
        "        super(RNN_model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers, bidirectional=bidirectional, batch_first=True)\n",
        "        self.hidden_size = hidden_dim * get_directions(bidirectional)\n",
        "        self.linear = nn.Linear(hidden_dim * get_directions(bidirectional), output_dim)\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, hidden = self.rnn(embeddings)\n",
        "        output_concat = torch.cat([output[:, :, :self.hidden_size], output[:, :, self.hidden_size:]], dim=2) # concatenates outputs\n",
        "        logits = self.linear(output_concat[:, -1, :]) # the last output of the concatenated RNN is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "\n",
        "def get_directions(bidirectional):\n",
        "    return 2 if bidirectional else 1\n",
        "\n",
        "# CASE.A) [model: RNN, num_layers: 1, bidirectional: False]\n",
        "classifier, loss_fn, optimizer = setup_model(device, RNN_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 1, False, LEARNING_RATE, None, None)"
      ],
      "metadata": {
        "id": "hcWFZq-Q4DkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(classifier, loss_fn, optimizer, train_loader, epochs):\n",
        "    times = []\n",
        "    for i in range(1, epochs+1):\n",
        "        classifier.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        start_time = time.time()\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds = classifier(X)\n",
        "            loss = loss_fn(Y_preds, Y)\n",
        "            losses.append(loss.item())\n",
        "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "        epoch_time = time.time() - start_time\n",
        "        times.append(epoch_time)\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "    return sum(times)/len(times)\n",
        "\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)"
      ],
      "metadata": {
        "id": "-Tyx3S5lVKI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(classifier, loss_fn, test_loader, test_data):\n",
        "    classifier.eval()\n",
        "    with torch.no_grad():  # during evaluation we don't update the model's parameters\n",
        "        Y_actual, Y_preds, losses = [],[],[]\n",
        "        for X, Y in test_loader:\n",
        "            preds = classifier(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds.append(preds.argmax(dim=-1))\n",
        "        Y_actual, Y_preds = torch.cat(Y_actual), torch.cat(Y_preds)\n",
        "        misclass_data = detect_misclassification(test_data,Y_preds.detach().cpu().numpy())\n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy(), misclass_data  # returns mean loss, actual labels and predicted labels\n",
        "\n",
        "def detect_misclassification(test_data, y_pred):\n",
        "    misclass_data = defaultdict(list)\n",
        "    for i in range(len(test_data[\"labels\"])):\n",
        "        true_label = test_data[\"labels\"][i]\n",
        "        predicted_label = y_pred[i]\n",
        "        if true_label != predicted_label:\n",
        "            text = test_data[\"features\"][i]\n",
        "            misclass_data[true_label].append((text, predicted_label))\n",
        "    return misclass_data\n",
        "\n",
        "def to_dict(tuples_list):\n",
        "    return {'features': [d[1] for d in tuples_list], 'labels': [d[0] for d in tuples_list]}\n",
        "\n",
        "_, Y_actual, Y_preds, misclass_data_1UniRNN = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))"
      ],
      "metadata": {
        "id": "RQgh1Om0Vo7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "l8anMr3JWJzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.B) [model: RNN, num_layers: 1, bidirectional: True]\n",
        "classifier, loss_fn, optimizer = setup_model(device, RNN_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 1, True, LEARNING_RATE, None, None)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_1BiRNN = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "O71o-crdWK8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.C) [model: RNN, num_layers: 2, bidirectional: True]\n",
        "classifier, loss_fn, optimizer = setup_model(device, RNN_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 2, True, LEARNING_RATE, None, None)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_2BiRNN = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "UvvcYm6XWYJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_model(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, bidirectional, output_dim, none, freeze):\n",
        "        super(LSTM_model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers, bidirectional=bidirectional, batch_first=True)\n",
        "        self.hidden_size = hidden_dim * get_directions(bidirectional)\n",
        "        self.linear = nn.Linear(hidden_dim * get_directions(bidirectional), output_dim)\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, (hidden, cell) = self.lstm(embeddings)\n",
        "        output_concat = torch.cat([output[:, :, :self.hidden_size], output[:, :, self.hidden_size:]], dim=2) # concatenates outputs\n",
        "        logits = self.linear(output_concat[:, -1, :]) # the last output of the concatenated LSTM is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "\n",
        "# CASE.D) [model: LSTM, num_layers: 1, bidirectional: False]\n",
        "classifier, loss_fn, optimizer = setup_model(device, LSTM_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 1, False, LEARNING_RATE, None, None)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_1UniLSTM = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "VSt0QpUnWcAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.E) [model: LSTM, num_layers: 1, bidirectional: True]\n",
        "classifier, loss_fn, optimizer = setup_model(device, LSTM_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 1, True, LEARNING_RATE, None, None)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_1BiLSTM = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "nOCwPy5pWfFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.F) [model: LSTM, num_layers: 2, bidirectional: True]\n",
        "classifier, loss_fn, optimizer = setup_model(device, LSTM_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 2, True, LEARNING_RATE, None, None)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_2BiLSTM = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "4UHALeleXFLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize(models, accuracies, parameters, time_costs):\n",
        "    pt = PrettyTable(field_names=[f\"\\033[1m{field}\\033[0m\" for field in [\"Model\", \"Accuracy\", \"Parameters\", \"Time Cost\"]])\n",
        "    [pt.add_row([model, accuracies[i], parameters[i], time_costs[i]]) for i, model in enumerate(models)]\n",
        "    print(pt)\n",
        "\n",
        "visualize(models, accuracies, parameters, time_costs)\n",
        "#########################################################################################################################################################"
      ],
      "metadata": {
        "id": "yUBnnjL2XF67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_results(models, misclassified):\n",
        "    common_misclass_data = defaultdict(list)\n",
        "    for true_label in misclassified[0].keys():\n",
        "        for text, label in misclassified[0][true_label]:\n",
        "            labels = [label] + [next((p for t, p in model[true_label] if t == text), '') for model in misclassified[1:]]\n",
        "            common_misclass_data[true_label].append((text, labels)) if all(labels) else None\n",
        "    count_times(common_misclass_data)\n",
        "    get_top_pair(common_misclass_data)\n",
        "    get_random_text(models, common_misclass_data)\n",
        "\n",
        "def count_times(common_misclass_data):\n",
        "    misclass_counts = {true_label: len(misclass_tuple) for true_label, misclass_tuple in common_misclass_data.items()}\n",
        "    pt = PrettyTable(field_names=[f\"\\033[1m{field}\\033[0m\" for field in [\"True Label\", \"Misclassification Times\"]])\n",
        "    [pt.add_row([true_label, count]) for true_label, count in misclass_counts.items()]\n",
        "    print(pt)\n",
        "\n",
        "def get_top_pair(common_misclass_data):\n",
        "    misclass_freqs = defaultdict(int)\n",
        "    for true_label, values in common_misclass_data.items():\n",
        "        for text, pred_labels in values:\n",
        "            for pl in pred_labels:\n",
        "                misclass_freqs[(true_label, pl)] += 1\n",
        "    max_tuple, max_count = max(misclass_freqs.items(), key=lambda x: x[1])\n",
        "    print(\"\\033[1m\" + \"Most common Misclassification Pair:\" + \"\\033[0m\")\n",
        "    pt = PrettyTable(field_names=[f\"\\033[1m{field}\\033[0m\" for field in [\"True Label\", \"Predicted Label\", \"Frequency\"]])\n",
        "    pt.add_row([max_tuple[0], max_tuple[1], max_count])\n",
        "    print(pt)\n",
        "\n",
        "def get_random_text(models, common_misclass_data):\n",
        "    rand_true_label = random.choice(list(common_misclass_data.keys()))\n",
        "    rand_misclass_tuple = random.choice(common_misclass_data[rand_true_label])\n",
        "    print(\"\\033[1m\" + \"Random Text: \" + \"\\033[0m\" + rand_misclass_tuple[0] + \"\\033[1m\" + \"\\nTrue Label: \" + \"\\033[0m\" + str(rand_true_label))\n",
        "    pt = PrettyTable(field_names=[f\"\\033[1m{field}\\033[0m\" for field in [\"Model\", \"Prediction\"]])\n",
        "    [pt.add_row([model, rand_misclass_tuple[1][idx]]) for idx, model in enumerate(models)]\n",
        "    print(pt)\n",
        "\n",
        "analyze_results(models, [misclass_data_1UniRNN, misclass_data_1BiRNN, misclass_data_2BiRNN, misclass_data_1UniLSTM, misclass_data_1BiLSTM, misclass_data_2BiLSTM])\n",
        "#########################################################################################################################################################"
      ],
      "metadata": {
        "id": "-fOMMmND4qD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_WORDS = 50\n",
        "train_loader, test_loader = generate_loader(train_dataset, MAX_WORDS, BATCH_SIZE, True), generate_loader(test_dataset, MAX_WORDS, BATCH_SIZE, False)"
      ],
      "metadata": {
        "id": "98l70jeOcwL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.A) [model: RNN, num_layers: 1, bidirectional: False]\n",
        "classifier, loss_fn, optimizer = setup_model(device, RNN_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 1, False, LEARNING_RATE, None, None)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_1UniRNN = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "zZVuqczklSkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.B) [model: RNN, num_layers: 1, bidirectional: True]\n",
        "classifier, loss_fn, optimizer = setup_model(device, RNN_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 1, True, LEARNING_RATE, None, None)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_1BiRNN = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "fy8-aqV2lkdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.C) [model: RNN, num_layers: 2, bidirectional: True]\n",
        "classifier, loss_fn, optimizer = setup_model(device, RNN_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 2, True, LEARNING_RATE, None, None)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_2BiRNN = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "84ikzBdblmi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.D) [model: LSTM, num_layers: 1, bidirectional: False]\n",
        "classifier, loss_fn, optimizer = setup_model(device, LSTM_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 1, False, LEARNING_RATE, None, None)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_1UniLSTM = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "I8pE3kK_lonM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.E) [model: LSTM, num_layers: 1, bidirectional: True]\n",
        "classifier, loss_fn, optimizer = setup_model(device, LSTM_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 1, True, LEARNING_RATE, None, None)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_1BiLSTM = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "81Htqcx5lqQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.F) [model: LSTM, num_layers: 2, bidirectional: True]\n",
        "classifier, loss_fn, optimizer = setup_model(device, LSTM_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 2, True, LEARNING_RATE, None, None)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_2BiLSTM = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "VR_0KAAslroM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(models, accuracies, parameters, time_costs)\n",
        "#########################################################################################################################################################"
      ],
      "metadata": {
        "id": "7eGJ1IKlls3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_WORDS = 25\n",
        "train_loader, test_loader = generate_loader(train_dataset, MAX_WORDS, BATCH_SIZE, True), generate_loader(test_dataset, MAX_WORDS, BATCH_SIZE, False)\n",
        "models = [\"1Uni-preRNN\", \"1Bi-preRNN\", \"2Bi-preRNN\", \"1Uni-preLSTM\", \"1Bi-preLSTM\", \"2Bi-preLSTM\"]; accuracies = []; parameters = []; time_costs = []"
      ],
      "metadata": {
        "id": "GHExRVqwBCTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embeddings(path, vocab, dimension):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "    embeddings = torch.zeros(len(vocab), dimension)\n",
        "    for line in lines:\n",
        "        word, vec = line.strip().split(' ', 1)\n",
        "        if word in vocab:\n",
        "            embeddings[vocab[word]] = torch.tensor([float(x) for x in vec.split()])\n",
        "    return embeddings\n",
        "\n",
        "embeddings = load_embeddings(\"glove.6B.100d.txt\", vocab, EMBEDDING_DIM)"
      ],
      "metadata": {
        "id": "UWjOylygH6t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class pretrained_RNN_model(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, bidirectional, output_dim, embeddings, freeze):\n",
        "        super(pretrained_RNN_model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.embedding_layer.weight.data.copy_(embeddings)\n",
        "        self.embedding_layer.weight.requires_grad = freeze  # freezes the weights of the embedding layer\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers, bidirectional=bidirectional, batch_first=True)\n",
        "        self.hidden_size = hidden_dim * get_directions(bidirectional)\n",
        "        self.linear = nn.Linear(hidden_dim * get_directions(bidirectional), output_dim)\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, hidden = self.rnn(embeddings)\n",
        "        output_concat = torch.cat([output[:, :, :self.hidden_size], output[:, :, self.hidden_size:]], dim=2) # concatenates outputs\n",
        "        logits = self.linear(output_concat[:, -1, :]) # the last output of the concatenated RNN is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "\n",
        "# CASE.A) [model: RNN, num_layers: 1, bidirectional: False]\n",
        "classifier, loss_fn, optimizer = setup_model(device, pretrained_RNN_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 1, False, LEARNING_RATE, embeddings, False)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_1UniRNN = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "8J5AhAaAIlIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.B) [model: RNN, num_layers: 1, bidirectional: True]\n",
        "classifier, loss_fn, optimizer = setup_model(device, pretrained_RNN_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 1, True, LEARNING_RATE, embeddings, False)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_1BiRNN = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "f9tKhkGzI1KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.C) [model: RNN, num_layers: 2, bidirectional: True]\n",
        "classifier, loss_fn, optimizer = setup_model(device, pretrained_RNN_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 2, True, LEARNING_RATE, embeddings, False)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_2BiRNN = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "LIXnc8_ZbyZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class pretrained_LSTM_model(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, bidirectional, output_dim, embeddings, freeze):\n",
        "        super(pretrained_LSTM_model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.embedding_layer.weight.data.copy_(embeddings)\n",
        "        self.embedding_layer.weight.requires_grad = freeze  # freezes the weights of the embedding layer\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers, bidirectional=bidirectional, batch_first=True)\n",
        "        self.hidden_size = hidden_dim * get_directions(bidirectional)\n",
        "        self.linear = nn.Linear(hidden_dim * get_directions(bidirectional), output_dim)\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, (hidden, cell) = self.lstm(embeddings)\n",
        "        output_concat = torch.cat([output[:, :, :self.hidden_size], output[:, :, self.hidden_size:]], dim=2) # concatenates outputs\n",
        "        logits = self.linear(output_concat[:, -1, :]) # the last output of the concatenated LSTM is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "\n",
        "# CASE.D) [model: LSTM, num_layers: 1, bidirectional: False]\n",
        "classifier, loss_fn, optimizer = setup_model(device, pretrained_LSTM_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 1, False, LEARNING_RATE, embeddings, False)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_1UniLSTM = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "DuAjQTnfbzcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.E) [model: LSTM, num_layers: 1, bidirectional: True]\n",
        "classifier, loss_fn, optimizer = setup_model(device, pretrained_LSTM_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 1, True, LEARNING_RATE, embeddings, False)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_1BiLSTM = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "X3r85qgZb3x2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.F) [model: LSTM, num_layers: 2, bidirectional: True]\n",
        "classifier, loss_fn, optimizer = setup_model(device, pretrained_LSTM_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 2, True, LEARNING_RATE, embeddings, False)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_2BiLSTM = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "g6tfLEBsb5Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(models, accuracies, parameters, time_costs)\n",
        "#########################################################################################################################################################"
      ],
      "metadata": {
        "id": "R2Axb3sGcBLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies = []; parameters = []; time_costs = []"
      ],
      "metadata": {
        "id": "81cVjb6_oELE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.A) [model: RNN, num_layers: 1, bidirectional: False]\n",
        "classifier, loss_fn, optimizer = setup_model(device, pretrained_RNN_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 1, False, LEARNING_RATE, embeddings, True)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_1UniRNN = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "sMz3A_uncE62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.B) [model: RNN, num_layers: 1, bidirectional: True]\n",
        "classifier, loss_fn, optimizer = setup_model(device, pretrained_RNN_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 1, True, LEARNING_RATE, embeddings, True)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_1BiRNN = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "WfxHL8sqnskl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.C) [model: RNN, num_layers: 2, bidirectional: True]\n",
        "classifier, loss_fn, optimizer = setup_model(device, pretrained_RNN_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 2, True, LEARNING_RATE, embeddings, True)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_2BiRNN = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "1ex0PZTZnugO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.D) [model: LSTM, num_layers: 1, bidirectional: False]\n",
        "classifier, loss_fn, optimizer = setup_model(device, pretrained_LSTM_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 1, False, LEARNING_RATE, embeddings, True)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_1UniLSTM = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "vm8jL4NBnwQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.E) [model: LSTM, num_layers: 1, bidirectional: True]\n",
        "classifier, loss_fn, optimizer = setup_model(device, pretrained_LSTM_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 1, True, LEARNING_RATE, embeddings, True)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_1BiLSTM = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "E90rAICnnxln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.F) [model: LSTM, num_layers: 2, bidirectional: True]\n",
        "classifier, loss_fn, optimizer = setup_model(device, pretrained_LSTM_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 2, True, LEARNING_RATE, embeddings, True)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_2BiLSTM = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "KHHxnoO7nyvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(models, accuracies, parameters, time_costs)\n",
        "#########################################################################################################################################################"
      ],
      "metadata": {
        "id": "t5bRajBAn0Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\"1Uni-RNN\", \"1Bi-RNN\", \"2Bi-RNN\", \"1Uni-LSTM\", \"1Bi-LSTM\", \"2Bi-LSTM\"]; classes = [\"Positive\", \"Negative\"]; accuracies = []; parameters = []; time_costs = []\n",
        "train_dataset, test_dataset = load_dataset(\"IMDB Dataset.csv\", [\"review\"], \"sentiment\", 80, \"start\"), load_dataset(\"IMDB Dataset.csv\", [\"review\"], \"sentiment\", 20, \"end\")"
      ],
      "metadata": {
        "id": "0weqTUexpewP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_labels(dataset, categorical, numerical):\n",
        "    mapping = {categorical[0]: numerical[0], categorical[1]: numerical[1]}\n",
        "    return [(mapping[label], text) for label, text in dataset]\n",
        "\n",
        "train_dataset, test_dataset = replace_labels(train_dataset, [\"negative\", \"positive\"], [1,2]), replace_labels(test_dataset, [\"negative\", \"positive\"], [1,2])"
      ],
      "metadata": {
        "id": "9ci7Rp2qqCXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, test_loader = generate_loader(train_dataset, MAX_WORDS, BATCH_SIZE, True), generate_loader(test_dataset, MAX_WORDS, BATCH_SIZE, False)\n",
        "vocab = build_vocab([train_dataset, test_dataset], MIN_FREQ, PADDED, UNKNOWN)"
      ],
      "metadata": {
        "id": "sqH7Myk2qHBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.A) [model: RNN, num_layers: 1, bidirectional: False]\n",
        "classifier, loss_fn, optimizer = setup_model(device, RNN_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 1, False, LEARNING_RATE, None, None)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_1UniRNN = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "ZH2plC5bqQAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.B) [model: RNN, num_layers: 1, bidirectional: True]\n",
        "classifier, loss_fn, optimizer = setup_model(device, RNN_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 1, True, LEARNING_RATE, None, None)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_1BiRNN = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "-a7pOcyXqYyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.C) [model: RNN, num_layers: 2, bidirectional: True]\n",
        "classifier, loss_fn, optimizer = setup_model(device, RNN_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 2, True, LEARNING_RATE, None, None)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_2BiRNN = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "eVCDnakhqafD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.D) [model: LSTM, num_layers: 1, bidirectional: False]\n",
        "classifier, loss_fn, optimizer = setup_model(device, LSTM_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 1, False, LEARNING_RATE, None, None)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_1UniLSTM = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "-8h2Gwcoqb3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.E) [model: LSTM, num_layers: 1, bidirectional: True]\n",
        "classifier, loss_fn, optimizer = setup_model(device, LSTM_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 1, True, LEARNING_RATE, None, None)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_1BiLSTM = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "W5TjwnI6qdJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE.F) [model: LSTM, num_layers: 2, bidirectional: True]\n",
        "classifier, loss_fn, optimizer = setup_model(device, LSTM_model, classes, vocab, EMBEDDING_DIM, HIDDEN_DIM, 2, True, LEARNING_RATE, None, None)\n",
        "time_cost = train_model(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "_, Y_actual, Y_preds, misclass_data_2BiLSTM = evaluate_model(classifier, loss_fn, test_loader, to_dict(test_dataset))\n",
        "accuracies.append(accuracy_score(Y_actual, Y_preds))\n",
        "parameters.append(count_parameters(classifier))\n",
        "time_costs.append(time_cost)"
      ],
      "metadata": {
        "id": "oacXOs7nqePr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(models, accuracies, parameters, time_costs)"
      ],
      "metadata": {
        "id": "Cnf3hB1-qfe1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSYAqCtPtereuhqmOG/Qe2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
