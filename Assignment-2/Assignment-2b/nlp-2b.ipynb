{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddf3169f",
   "metadata": {},
   "source": [
    "# NLP-Various Implementations | Text Classification using TF-IDF Vectorization\n",
    "\n",
    "**Overview:** In this part of the project, I implemented a machine learning model that classifies text using Naive Bayes and Support Vector Machines (SVM) algorithms. The TfidfVectorizer function transforms text into feature vectors, and the train_classifier() function trains the classifier. The predict() function outputs predicted labels, whereas the run_model() function returns accuracy, dimensionality, time cost, and misclassification data for later use. The analyze_results() function outputs the most common misclassification pair and a random text with corresponding true and predicted labels for each model. The code also provides a way to compare the performance of Naive Bayes and SVM using various parameters.\n",
    "\n",
    "## 1. Import all the necessary modules\n",
    "\n",
    "**Briefly:** `csv` library provides functionality for working with Comma Separated Value (CSV) files, `time` provides functions for working with time-related tasks, `random` provides tools for generating random numbers, `defaultdict` provides a way to create a dictionary with default values for nonexistent keys, `PrettyTable` provides a way to display data in a table format, `TfidfVectorizer` is a function from `sklearn.feature_extraction.text` that transforms text into feature vectors, `MultinomialNB` and `LinearSVC` are machine learning algorithms from `sklearn.naive_bayes` and `sklearn.svm respectively`, used for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee8b378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import random\n",
    "from prettytable import PrettyTable\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2904fe01",
   "metadata": {},
   "source": [
    "## 2. Load the training and testing datasets\n",
    "\n",
    "The function load_dataset() takes in the path of a CSV file containing text data and returns a dictionary with the features and labels. The features are a concatenation of the second and third columns of each row in the CSV file, while the labels correspond to the first column. The function is used to load the training and test data from two CSV files in the specified file paths and store them in variables named train_data and test_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a08fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = list(csv.reader(f))\n",
    "    return {\"features\": [row[1] + ' ' + row[2] for row in data[1:]], \"labels\": [row[0] for row in data[1:]]}\n",
    "\n",
    "train_data = load_dataset(\"C:/Users/natalia/pyproj/nlp-proj/assignment-2b/train.csv\")\n",
    "test_data = load_dataset(\"C:/Users/natalia/pyproj/nlp-proj/assignment-2b/test.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffb93aa",
   "metadata": {},
   "source": [
    "## 3. Build and run the text classification model\n",
    "\n",
    "The run_model function takes in four arguments and extracts features from the training and test data using TfidfVectorizer. It then trains a classifier using the extracted features and returns the accuracy score, dimensionality, time taken for training and testing, and misclassification data.\n",
    "\n",
    "* **Step.1:** the run_model function is called with four arguments: train_data, test_data, n, approach, and classifier. In the meantime, the timer for calculating the time taken to run the model starts counting.\n",
    "* **Step.2:** the extract_features function is called within run_model, to extract features from the train_data and test_data using TfidfVectorizer. The extracted features are assigned to variables X_train, X_test, and vocab. The labels from train_data and test_data are assigned to y_train and y_test, respectively.\n",
    "* **Step.3:** the train_classifier function is also called within run_model, with X_train, y_train, and classifier as arguments, to train a classifier using the provided training data. The timer is then stopped to get the time taken for training the classifier.\n",
    "* **Step.4:** the detect_misclassification function is called within run_model as well, with test_data and the predicted labels from the classifier's predict function as arguments to calculate misclassification data. The predict function takes classifier and X_test as arguments and returns predicted labels for the test data.\n",
    "* **Step.5:** finally, the run_model function returns four values: accuracy score, dimensionality, time taken for training and testing, and misclassification data. Accuracy represents the accuracy score of the classifier on the test data, dimensionality is the number of unique features in the extracted features, time_cost is the time taken to train the classifier and misclass_data_mnv1w is a dictionary containing misclassification data.\n",
    "\n",
    "### 3.1. Multinomial Naïve Bayes using tf-idf word uni-grams\n",
    "\n",
    "The run_model() function is called with five arguments: train_data, test_data, 1 for the n-gram range, \"word\" for the tokenization approach and an instance of the MultinomialNB class as the classification algorithm. The function extracts features using TF-IDF with word uni-grams, trains a Multinomial Naïve Bayes classifier, and calculates the accuracy score, dimensionality, time cost, and misclassification data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9f19052",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_model(train_data, test_data, n, approach, classifier):\n",
    "    start = time.time()\n",
    "    X_train, X_test, vocab = extract_features(train_data[\"features\"], test_data[\"features\"], n, approach) # (i, j) entries represent the presence and frequency of the j-th feature (word) in the i-th document - the values in each entry represent the corresponding term frequency-inverse document frequency (tf-idf) score\n",
    "    y_train, y_test = train_data[\"labels\"], test_data[\"labels\"]\n",
    "    clf = train_classifier(X_train, y_train, classifier)\n",
    "    end = time.time()\n",
    "    misclass_data = detect_misclassification(test_data, predict(classifier, X_test))\n",
    "    return clf.score(X_test, y_test), len(vocab), end - start, misclass_data\n",
    "\n",
    "def extract_features(train_text, test_text, n, approach):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(n,n), lowercase=True, analyzer=approach) # where (n,n): the lower and upper bounds of the range of n-grams to be extracted\n",
    "    return vectorizer.fit_transform(train_text), vectorizer.transform(test_text), vectorizer.vocabulary_\n",
    "\n",
    "def train_classifier(X_train, y_train, classifier):\n",
    "    return classifier.fit(X_train, y_train)\n",
    "\n",
    "def predict(classifier, X_test):\n",
    "    return classifier.predict(X_test)\n",
    "\n",
    "def detect_misclassification(test_data, y_pred):\n",
    "    misclass_data = defaultdict(list)\n",
    "    for i in range(len(test_data[\"labels\"])):\n",
    "        true_label = test_data[\"labels\"][i]\n",
    "        predicted_label = y_pred[i]\n",
    "        if true_label != predicted_label:\n",
    "            text = test_data[\"features\"][i]\n",
    "            misclass_data[true_label].append((text, predicted_label))\n",
    "    return misclass_data\n",
    "\n",
    "accuracy, dimensionality, time_cost, misclass_data_mnv1w = run_model(train_data, test_data, 1, \"word\", MultinomialNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9035d046",
   "metadata": {},
   "source": [
    "The visualize function takes a model name, accuracy, dimensionality, and time cost as inputs, and prints a pretty table showing the model's performance. In this case, it is used to display the results of this model: Multinomial Naïve Bayes using tf-idf word uni-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f813627e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mMultinomial Naïve Bayes using tf-idf word uni-grams:\u001b[0m\n",
      "+--------------------+----------------+-------------------+\n",
      "|      \u001b[1mAccuracy\u001b[0m      | \u001b[1mDimensionality\u001b[0m |     \u001b[1mTime Cost\u001b[0m     |\n",
      "+--------------------+----------------+-------------------+\n",
      "| 0.9022368421052631 |     64999      | 6.860435485839844 |\n",
      "+--------------------+----------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "def visualize(model, accuracy, dimensionality, time_cost):\n",
    "    print(f\"\\033[1m{model}:\\033[0m\")\n",
    "    pt = PrettyTable(field_names=[f\"\\033[1m{field}\\033[0m\" for field in [\"Accuracy\", \"Dimensionality\", \"Time Cost\"]])\n",
    "    pt.add_row([accuracy, dimensionality, time_cost])\n",
    "    print(pt)\n",
    "\n",
    "visualize(\"Multinomial Naïve Bayes using tf-idf word uni-grams\", accuracy, dimensionality, time_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c5bd75",
   "metadata": {},
   "source": [
    "### 3.2. Multinomial Naïve Bayes using tf-idf character tri-grams\n",
    "\n",
    "The run_model() function is called with five arguments: train_data, test_data, 3 for the n-gram range, \"char\" for the tokenization approach and an instance of the MultinomialNB class as the classification algorithm. The function extracts features using TF-IDF with character tri-grams, trains a Multinomial Naïve Bayes classifier, and calculates the accuracy score, dimensionality, time cost, and misclassification data. It then visualizes the accuracy, dimensionality, and time cost of the model using the \"visualize\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36a7148e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mMultinomial Naïve Bayes using tf-idf character tri-grams:\u001b[0m\n",
      "+--------------------+----------------+--------------------+\n",
      "|      \u001b[1mAccuracy\u001b[0m      | \u001b[1mDimensionality\u001b[0m |     \u001b[1mTime Cost\u001b[0m      |\n",
      "+--------------------+----------------+--------------------+\n",
      "| 0.8686842105263158 |     31074      | 24.951099157333374 |\n",
      "+--------------------+----------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "accuracy, dimensionality, time_cost, misclass_data_mnv3c = run_model(train_data, test_data, 3, \"char\", MultinomialNB())\n",
    "visualize(\"Multinomial Naïve Bayes using tf-idf character tri-grams\", accuracy, dimensionality, time_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf403a0",
   "metadata": {},
   "source": [
    "### 3.3. Support Vector Machines using tf-idf word uni-grams\n",
    "\n",
    "The run_model() function is called with five arguments: train_data, test_data, 1 for the n-gram range, \"word\" for the tokenization approach and an instance of the LinearSVC class as the classification algorithm with C parameter set to 1. The function extracts features using TF-IDF with word uni-grams, trains a Support Vector Machines classifier, and calculates the accuracy score, dimensionality, time cost, and misclassification data. It then visualizes the accuracy, dimensionality, and time cost of the model using the \"visualize\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "326a7a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSupport Vector Machines using tf-idf word uni-grams:\u001b[0m\n",
      "+--------------------+----------------+-------------------+\n",
      "|      \u001b[1mAccuracy\u001b[0m      | \u001b[1mDimensionality\u001b[0m |     \u001b[1mTime Cost\u001b[0m     |\n",
      "+--------------------+----------------+-------------------+\n",
      "| 0.9196052631578947 |     64999      | 11.84224820137024 |\n",
      "+--------------------+----------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "accuracy, dimensionality, time_cost, misclass_data_svm1w = run_model(train_data, test_data, 1, \"word\", LinearSVC(C=1))\n",
    "visualize(\"Support Vector Machines using tf-idf word uni-grams\", accuracy, dimensionality, time_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f393bf",
   "metadata": {},
   "source": [
    "### 3.4. Support Vector Machines using tf-idf character tri-grams\n",
    "\n",
    "The run_model() function is called with five arguments: train_data, test_data, 3 for the n-gram range, \"char\" for the tokenization approach, and an instance of the LinearSVC class as the classification algorithm with C parameter set to 1. The function extracts features using TF-IDF with character tri-grams, trains a Support Vector Machines classifier, and calculates the accuracy score, dimensionality, time cost, and misclassification data. It then visualizes the accuracy, dimensionality, and time cost of the model using the \"visualize\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1418dc5d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSupport Vector Machines using tf-idf character tri-grams:\u001b[0m\n",
      "+--------------------+----------------+-------------------+\n",
      "|      \u001b[1mAccuracy\u001b[0m      | \u001b[1mDimensionality\u001b[0m |     \u001b[1mTime Cost\u001b[0m     |\n",
      "+--------------------+----------------+-------------------+\n",
      "| 0.9121052631578948 |     31074      | 38.08852005004883 |\n",
      "+--------------------+----------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "accuracy, dimensionality, time_cost, misclass_data_svm3c = run_model(train_data, test_data, 3, \"char\", LinearSVC(C=1))\n",
    "visualize(\"Support Vector Machines using tf-idf character tri-grams\", accuracy, dimensionality, time_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceaea44",
   "metadata": {},
   "source": [
    "## 4. Analyze the results of the models\n",
    "\n",
    "The analyze_results function analyzes the misclassification data of multiple models by creating a common_misclass_data dictionary to store the texts that were misclassified by all the models and calls three helper functions to count the number of misclassified texts for each true label, determine the most common misclassification pair, and display a random misclassified text and its predictions made by each model.\n",
    "\n",
    "* **Step.1:** the analyze_results function starts by creating common_misclass_data dictionary to store the texts that were misclassified by all the models. For each text that was misclassified, it looks at the true label and stores the text along with the predicted labels from all the models in a list. It then calls three helper functions, count_times(), get_top_pair(), and get_random_text(), to further analyze the data in terms of misclassification.\n",
    "* **Step.2:** the count_times() function is called within analyze_results. It takes as input the common_misclass_data dictionary and the list of classes that contains all the possible labels in the dataset. It first counts the number of misclassified texts for each true label by computing the length of the corresponding dictionary value (which is a list of tuples where each tuple contains a misclassified text and its predicted labels from all models). Then, it creates a pretty table to display the number of common misclassified texts for each true label across all models.\n",
    "* **Step.3:** the get_top_pair() function is also called within analyze_results. It takes as input the common_misclass_data dictionary and the list of classes that contains all the possible labels in the dataset. It then iterates through each true label and the corresponding misclassified texts and counts the number of times each pair of true label and predicted label occurs in the list of misclassified texts. It then determines the pair with the highest count and prints it as the most common misclassification pair. Finally, the function creates a PrettyTable object to display the frequencies of all the misclassification pairs, sorted in descending order by frequency.\n",
    "* **Step.4:** the get_random_text() function is called within analyze_results as well. It takes as input the list of all the models, the common_misclass_data dictionary, and the list of classes that contains all the possible labels in the dataset. It randomly selects a misclassified text and displays it along with its true label. It then shows the predictions made by each model in a table. The purpose of this function is to allow the user to see an example of a misclassified text and the various predictions made by the models for that text.\n",
    "\n",
    "> the to_category() function takes a numerical label as input, along with a list of classes, and returns the corresponding string label for that numerical value.\n",
    "\n",
    "The analyze_results function is called with four arguments: a list of category labels ([\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]), a list of model names ([\"mnv1w\", \"mnv3c\", \"svm1w\", \"svm3c\"]) and a list of misclassification data for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "973ed8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCommon Misclassified Texts per Class:\u001b[0m\n",
      "+------------+---------------------+\n",
      "| \u001b[1mTrue Label\u001b[0m | \u001b[1mMisclassified Texts\u001b[0m |\n",
      "+------------+---------------------+\n",
      "|  Sci/Tech  |          85         |\n",
      "|  Business  |         135         |\n",
      "|   World    |         112         |\n",
      "|   Sports   |          9          |\n",
      "+------------+---------------------+\n",
      "\n",
      "\u001b[1mMost common Misclassification Pair:\u001b[0m (Business, Sci/Tech)\n",
      "+------------+-----------------+-----------+\n",
      "| \u001b[1mTrue Label\u001b[0m | \u001b[1mPredicted Label\u001b[0m | \u001b[1mFrequency\u001b[0m |\n",
      "+------------+-----------------+-----------+\n",
      "|  Business  |     Sci/Tech    |    381    |\n",
      "|  Sci/Tech  |     Business    |    206    |\n",
      "|   World    |     Business    |    194    |\n",
      "|   World    |      Sports     |    144    |\n",
      "|  Business  |      World      |    123    |\n",
      "|   World    |     Sci/Tech    |    110    |\n",
      "|  Sci/Tech  |      World      |    103    |\n",
      "|  Business  |      Sports     |     36    |\n",
      "|  Sci/Tech  |      Sports     |     31    |\n",
      "|   Sports   |      World      |     17    |\n",
      "|   Sports   |     Business    |     12    |\n",
      "|   Sports   |     Sci/Tech    |     7     |\n",
      "+------------+-----------------+-----------+\n",
      "\n",
      "\u001b[1mRandom Text: \u001b[0mInfineon to Pay a Fine in the Fixing Of Chip Prices Federal prosecutors announced on Wednesday that they had cracked a global cartel that had illegally fixed prices of memory chips in personal computers and servers for \u001b[1m\n",
      "True Label: \u001b[0mBusiness\n",
      "+-------+------------+\n",
      "| \u001b[1mModel\u001b[0m | \u001b[1mPrediction\u001b[0m |\n",
      "+-------+------------+\n",
      "| mnv1w |  Sci/Tech  |\n",
      "| mnv3c |  Sci/Tech  |\n",
      "| svm1w |  Sci/Tech  |\n",
      "| svm3c |  Sci/Tech  |\n",
      "+-------+------------+\n"
     ]
    }
   ],
   "source": [
    "def count_times(classes, common_misclass_data):\n",
    "    misclass_counts = {true_label: len(misclass_tuples) for true_label, misclass_tuples in common_misclass_data.items()}\n",
    "    pt = PrettyTable(field_names=[f\"\\033[1m{field}\\033[0m\" for field in [\"True Label\", \"Misclassified Texts\"]])\n",
    "    [pt.add_row([to_category(true_label, classes), count]) for true_label, count in misclass_counts.items()]\n",
    "    print(\"\\033[1mCommon Misclassified Texts per Class:\\033[0m\")\n",
    "    print(pt)\n",
    "\n",
    "def get_top_pair(classes, common_misclass_data):\n",
    "    misclass_freqs = defaultdict(int)\n",
    "    for true_label, values in common_misclass_data.items():\n",
    "        for text, pred_labels in values:\n",
    "            for pl in pred_labels:\n",
    "                misclass_freqs[(true_label, pl)] += 1\n",
    "    max_tuple, max_count = max(misclass_freqs.items(), key=lambda x: x[1])\n",
    "    sorted_tuples = sorted(misclass_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(f\"\\n\\033[1mMost common Misclassification Pair:\\033[0m ({to_category(max_tuple[0], classes)}, {to_category(max_tuple[1], classes)})\")\n",
    "    pt = PrettyTable(field_names=[f\"\\033[1m{field}\\033[0m\" for field in [\"True Label\", \"Predicted Label\", \"Frequency\"]])\n",
    "    [pt.add_row([to_category(tup[0], classes), to_category(tup[1], classes), count]) for tup, count in sorted_tuples]\n",
    "    print(pt)\n",
    "\n",
    "def get_random_text(classes, models, common_misclass_data):\n",
    "    rand_true_label = random.choice(list(common_misclass_data.keys()))\n",
    "    rand_misclass_tuple = random.choice(common_misclass_data[rand_true_label])\n",
    "    print(\"\\n\\033[1m\" + \"Random Text: \" + \"\\033[0m\" + rand_misclass_tuple[0] + \"\\033[1m\" + \"\\nTrue Label: \" + \"\\033[0m\" + to_category(rand_true_label, classes))\n",
    "    pt = PrettyTable(field_names=[f\"\\033[1m{field}\\033[0m\" for field in [\"Model\", \"Prediction\"]])\n",
    "    [pt.add_row([model, to_category(rand_misclass_tuple[1][idx], classes)]) for idx, model in enumerate(models)]\n",
    "    print(pt)\n",
    "\n",
    "def to_category(label, classes):\n",
    "    return classes[int(label) - 1]\n",
    "    \n",
    "def analyze_results(classes, models, misclassified):\n",
    "    common_misclass_data = defaultdict(list)\n",
    "    for true_label in misclassified[0].keys():\n",
    "        for text, label in misclassified[0][true_label]:\n",
    "            labels = [label] + [next((l for t, l in model[true_label] if t == text), '') for model in misclassified[1:]]\n",
    "            common_misclass_data[true_label].append((text, labels)) if all(labels) else None\n",
    "    count_times(classes, common_misclass_data)\n",
    "    get_top_pair(classes, common_misclass_data)\n",
    "    get_random_text(classes, models, common_misclass_data)\n",
    "\n",
    "analyze_results([\"World\", \"Sports\", \"Business\", \"Sci/Tech\"], [\"mnv1w\", \"mnv3c\", \"svm1w\", \"svm3c\"], [misclass_data_mnv1w, misclass_data_mnv3c, misclass_data_svm1w, misclass_data_svm3c])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
